{
  "content": "1.  \n2.  小米神策标签对接文档\n一、确认数据目录\n与神策研发确认好以下两个目录：\n业务方参与导入的标签所属的界面上的目录，可以有多个。\n数据传输到神策集群的目录，如约定好目录为： /tmp/user_tag_import_data/，上传数据时需要存放在： /tmp/user_tag_import_data\n/tag=${tagName}/20210715 目录下，并保证目录有读写权限，目录时间即为basetime（运算时间）\n二、执行操作\n1、创建标签\n小米方在界面上创建导入类型标签（如果有其他标签，可以放在统一的目录下）\n首次创建可以将信息格式化为yml，神策技术在服务器上创建，yml格式如下：\ntags_config.yml\ntags:\n  - name: \"user_tag_formal_cust_flag\"\n    cname: \"正式客户标志\"\n    data_type: \"STRING\"\n    dir: \"目录1\"\n  - name: \"user_tag_cust_sts_cd\"\n    cname: \"客户状态码\"\n    data_type: \"STRING\"\n    dir: \"目录1\"\n  - name: \"user_tag_cust_sts\"\n    cname: \"客户状态\"\n    data_type: \"NUMBER\"\n    dir: \"/产品持有/个贷\"\n2、生成数据\nspark 生成  parquet 样例见附件 spark_demo.tar\n数据源目前仅支持指定格式的 PARQUET 文件\nfile schema:              spark_schema\n--------------------------------------------------------------------------------\ndistinct_id:              OPTIONAL BINARY L:STRING R:0 D:1\nuser_tag_test_number_0:   OPTIONAL DOUBLE R:0 D:1\nuser_tag_test_string_0:   OPTIONAL BINARY L:STRING R:0 D:1\nuser_tag_test_bool_0:     OPTIONAL BOOLEAN R:0 D:1\nuser_tag_test_datetime_0: OPTIONAL INT64 R:0 D:1\nuser_tag_test_list_0:     OPTIONAL BINARY L:STRING R:0 D:1\n \nrow group 1:              RC:7 TS:593 OFFSET:4\n--------------------------------------------------------------------------------\ndistinct_id:               BINARY GZIP DO:0 FPO:4 SZ:72 /68/0 .94 VC:7 ENC:BIT_PACKED,RLE,PLAIN ST:[min: 1, max: 7, num_nulls: 0]\nuser_tag_test_number_0:    DOUBLE GZIP DO:0 FPO:76 SZ:106 /115/1 .08 VC:7 ENC:BIT_PACKED,RLE,PLAIN ST:[min: 1.0, max: 7.0, num_nulls: 1]\nuser_tag_test_string_0:    BINARY GZIP DO:0 FPO:182 SZ:78 /87/1 .12 VC:7 ENC:BIT_PACKED,RLE,PLAIN ST:[min: str1, max: str7, num_nulls: 1]\nuser_tag_test_bool_0:      BOOLEAN GZIP DO:0 FPO:260 SZ:60 /40/0 .67 VC:7 ENC:BIT_PACKED,RLE,PLAIN ST:[min:  true, max:  true, num_nulls: 1]\nuser_tag_test_datetime_0:  INT64 GZIP DO:0 FPO:320 SZ:108 /115/1 .06 VC:7 ENC:BIT_PACKED,RLE,PLAIN ST:[min: 1624373383724, max: \n1624373383730, num_nulls: 1]\nuser_tag_test_list_0:      BINARY GZIP DO:0 FPO:428 SZ:131 /168/1 .28 VC:7 ENC:BIT_PACKED,RLE,PLAIN ST:[min: lista1 lista2, max: listg1 listg2 \nlistg3, num_nulls: 1]\n格式说明：\nPARQUET 文件中必须包含 \"distinct_id\" 列，且该列类型为 STRING 类型。其他列名必须为标签的英文名\n列名 标签类\n型PARQUET 列\n类型额外限制 举例\ndistinct_id -------- STRING 不能为空，长度最长 255，不包含隐藏字符，如（\" \\t\\b\\n\\r\\f\\'\\\"\n\\0\\1\\2\\3\\4\\5\\6\\7\\\\\"）17e9073a6deb4815904981c05e1b1cfa\nuser_tag_num\nberNUMBER DOUBLE 最大值 9E15，最小值 -9E15，最多保留小数点后三位 数值很大时，数据入库会有精度问题\n9000000000000000.666 -> \n9000000000000001\n900000000000000.666 -> \n900000000000000.6\n90000000000000.666 -> \n90000000000000.67\n9000000000000.666 -> \n9000000000000.666\nuser_tag_bool BOOL BOOLEAN 只能填 true true\n1.  \na.  \n2.  \na.  \n3.  \n1.  \n2.  \n3.  \n4.  \n5.  \n1.  \n2.  \n3.  \n4.  \n5.  \n6.  user_tag_datet\nimeDATETI\nMELONG 毫秒时间戳，最大值 7226553600000， 最小值 -2209017600000 1624432948751\nuser_tag_list LIST STRING 值使用换行符进行分割 list_value1\nlist_value2\nuser_tag_string STRING STRING 无限制 str_value\n文件结构可以使用 parquet-tools meta 进行核验\nParquet 文件大小建议等同于 HDFS 块大小 128MB，因在工具处理后文件会变得略大一些，建议控制在 64MB ~ 96 MB 左右为佳\n在文件生成后，可以先检查大小和当前行数，如果过大可以通过在 write 前设置来控制输出行数，从而控制大小。详见下方代码。\nParquet 文件压缩格式建议使用 gzip，gzip 是一种支持拆分的文件压缩格式，可以加大并行度\n初始化时可设置，详见下方代码。\nParquet 文件建议每个仅包含一个 Row Group，包含多个会造成读取速度降低，可以使用 meta 进行检验（parquet-tools meta xxx | grep 'row \ngroup' 显示多个则需要进行优化）\n    SparkSession sparkSession = SparkSession\n        .builder()\n        // 设置文件压缩格式为 gzip\n        .config(\"spark.sql.parquet.compression.codec\", \"gzip\")\n        .appName(\"DATA GEN GZIP\")\n        .getOrCreate();\n    Dataset<Row> dataFrame = sparkSession.createDataFrame(getTestData(), TestData.class);\n    dataFrame = dataFrame.map(getMapFunc(getSourceMeta()), RowEncoder.apply(getTargetMeta()));\n   \n    dataFrame.write()\n        // 控制文件行数，从而控制大小，如果文件生成出来就在 100M 左右就可以无视该参数\n        .option(\"maxRecordsPerFile\", 100000)\n        .mode(SaveMode.Overwrite)\n        .parquet(\"/sa/spark_test/data\");\n3、传输文件\n完成以下所有功能的 python 脚本见附件 file_push.py\n文件生成完毕后，生成标记文件（文件内每一行写一个本次导入的文件不含路径的全名），使用 distcp 推送到神策集群。以下步骤任意一步发生异常将在重\n试三次后发送邮件告警。\n删除数据对应日期的旧目录 \n创建数据对应日期的新目录\n给该目录授权\n推送文件\n推送成功标记文件\n4、数据导入【数据导入将由神策研发负责调试，以下为大致流程】\n神策集群定期扫描指定目录下，所有日期的文件夹，查找如 等目录，查找  /sa/import_data/tag=tag_name/20210715 _DATA_READY 文件\n按照日期排序，发现第一个就绪文件后，对该目录下的文件启动导入流程，启动导入后，任意一步发生异常都将触发报警\n将该文件夹重命名为【待导入文件夹】\n读取 _DATA_READY 文件，检查当前目录下是否存在所有待导入文件，检查完成后删除多余文件。若发现文件不全将终止导入并发出告警。\n扫描【约定好的目录】，生成标签导入配置，并启动导入。\n导入完成后，继续定期扫描其他目录，若有文件重新上传则重复 1~4\n对齐：为保证例行计算标签可以正确对齐以上导入的标签，导入时会添加自动对齐的参数，使自动计算的自动等待对齐日期导入标签，如20240703\n例行计算的标签会等待basetime为20240703的标签",
  "mtime": 1739343916.63439,
  "original_path": "C:\\Users\\MI\\Downloads\\11\\小米标签导入方案.pdf"
}